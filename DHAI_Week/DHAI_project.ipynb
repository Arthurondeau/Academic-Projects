{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "609fd2a8-6ed2-403a-b70b-08d9210cd267",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Image retrieval thanks to global image features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cf91d7-0547-4942-abf8-514842668d4a",
   "metadata": {},
   "source": [
    "A lot of papers are now using deep global features (and not anymore handcrafted features such as SIFTs, which fail in the case of strong style change) to perform image retrieval in historical documents, or pattern matching in artistics datasets - where it is necessary to retrieve images depending on their content, and independently of their \"style\".\n",
    "\n",
    "In our case, we are working on a datasets of ~5000 images of Astronomical diagrams (but also of illustrations, false detections, etc. - our segmentation method is not yet optimal!), that we want to explore.\n",
    "\n",
    "> *Is it possible to easily retrieve similar diagrams from a query one thanks to deep features? What is the best architecture / truncated final layer to do so? Wouldn't it be nice to also develop visualization / clustering tools to navigate in this dataset - and help historians of astronomy find inventive new formal connections between diagrams?*\n",
    "\n",
    "First of all, let's transform our images into features. For this, we use a `ResNet-50` architecture, and truncate it to output its `conv4` features - each image resized to `320×320`, and is represented by `20×20` features. To find the most \"similar\" images from a query one, we use [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity).\n",
    "\n",
    "Please play a little bit with the **network architecture**, the **final layer** choice, and try some **image retrieval queries** - to start understand what it is possible to do with deep features.\n",
    "\n",
    "First, let's play a little bit with some deep features representations.\n",
    "\n",
    "> ### *Improving results*\n",
    "> - Try to enhance input data (remove blank diagrams, annotate manualy manuscripts, etc.)\n",
    "> - Try different variables, models, truncation, etc.\n",
    "> - Augment the dataset by considering metadata on extracted diagrams (production date, geographical origin, manuscrit in which the diagram can be found, celectial object/phenomenon represented, etc.)\n",
    ">\n",
    "> ### *Interpreting results*\n",
    "> - What are telling visualisations?\n",
    "> - In what context could these techniques be useful?\n",
    "> - What are the limits of those analyses?\n",
    "\n",
    "## Resources\n",
    "\n",
    "### Historical images analysis\n",
    "\n",
    "#### Historical images segmentation\n",
    "\n",
    "- [**DocExtractor**](https://arxiv.org/abs/1804.10371): [GitHub](https://www.tmonnier.com/docExtractor/)\n",
    "\n",
    "#### Deep visual similarities for artistic/historical images retrieval\n",
    "\n",
    "- [Historical Image Analysis](https://imagine.enpc.fr/~shenx/HisImgAnalysis/)\n",
    "- [ImageCollation](https://imagine.enpc.fr/~shenx/ImageCollation/)\n",
    "- [ArtMiner](https://imagine.enpc.fr/~shenx/ArtMiner/)\n",
    "- [Aesthetic complexity and the evolution of visual art](https://arxiv.org/abs/2205.10271)\n",
    "- [Art History in the Eyes of the Machine](https://arxiv.org/abs/1801.07729)\n",
    "- [Astronomical diagrams: detection & grouping by formal ressemblance](https://docs.google.com/presentation/d/1Ebwqq5sH3rEdPkChyQek4f9rmGK9NzB8aaNe_W7JLLA/view?usp=sharing)\n",
    "- [Visual contagions](https://www.unige.ch/visualcontagions/expositions/jeu-de-paume-le-projet)\n",
    "\n",
    "### Models\n",
    "\n",
    "- **ResNet**: [Deep Residual Learning for Image Recognition](https://www.youtube.com/watch?v=GWt6Fu05voI)\n",
    "- **DeepDream**: [What Is Google Deep Dream?](https://www3.cs.stonybrook.edu/~cse352/T12talk.pdf)\n",
    "- **CLIP**: [Blog](https://openai.com/blog/clip/), [GitHub](https://github.com/openai/CLIP), [Paper](https://arxiv.org/abs/2103.00020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13200df3-f8ca-4f6d-adde-6dc944df09f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "%pip install torchvision\n",
    "%pip install numpy\n",
    "%pip install -U scikit-learn\n",
    "%pip install umap-learn\n",
    "%pip install matplotlib\n",
    "%pip install ftfy regex tqdm\n",
    "%pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "# ⚠️⚠️⚠️ Please restart your kernel after installation! ⚠️⚠️⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7d9c20-6823-40c4-9bac-411cda9aa5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL IMPORTS\n",
    "import re, glob, os, shutil\n",
    "from os.path import join, isfile, splitext, basename, dirname, exists\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import normalize\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ab15b7-d574-4d29-abbe-7dae497acae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS IMPORT\n",
    "import torchvision.models as models\n",
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148dbdbd-7206-4e66-82a2-b604ee269f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILITY FUNCTIONS\n",
    "def natural_sort_func(my_list):\n",
    "    def convert(text):\n",
    "        if text.isdigit():\n",
    "            return int(text)\n",
    "        return text.lower()\n",
    "\n",
    "    def alphanum_key(key):\n",
    "        return [convert(c) for c in re.split('([0-9]+)', key)]\n",
    "\n",
    "    return sorted(my_list, key=alphanum_key)\n",
    "\n",
    "def list_folder_images(folder, natural_sort=True):\n",
    "    image_types = ('jpg', 'tif', 'png', 'bmp')\n",
    "    paths = [join(root, file)\n",
    "            for root, dirs, files in os.walk(folder)\n",
    "            for file in files\n",
    "        if isfile(join(root, file)) and file.endswith(image_types)]\n",
    "    if natural_sort:\n",
    "        paths = natural_sort_func(paths)\n",
    "    return paths\n",
    "\n",
    "def get_images_list(folder):\n",
    "    images_path = list_folder_images(folder)\n",
    "    return [Image.open(image_path).convert('RGB') for image_path in images_path], images_path\n",
    "\n",
    "def zip_dir(dir_path):\n",
    "    shutil.make_archive(f\"{dir_path.split('/')[-1]}\", 'zip', dir_path)\n",
    "    \n",
    "def display_img(img_path, score=\"\"):\n",
    "    print(img_path)\n",
    "    print(score)\n",
    "    display(Image.open(img_path).convert('RGB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c545f95c-831d-49c1-9b2e-332ab381167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS TO COMPARE\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "resnet_50 = torch.hub.load('pytorch/vision:v0.6.0', 'resnet50', pretrained=True)\n",
    "# vgg = models.vgg16()\n",
    "# resnet18 = models.resnet18()\n",
    "clip_model, preprocess = clip.load('ViT-B/32', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7236b5f9-2783-4bb7-b578-9914a792a9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES AND SIMILARITY COMPUTATION FUNCTIONS\n",
    "def compute_features(model, images, size, batch_size, use_cuda=True):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((size, size)),\n",
    "        transforms.ToTensor(),\n",
    "        # normalize by mean and std of Imagenet\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    tensors = [preprocess(image) for image in images]\n",
    "    tensors = torch.stack(tensors, dim=0)\n",
    "    outputs = []\n",
    "    if torch.cuda.is_available() and use_cuda:\n",
    "        model.to('cuda')\n",
    "    model.eval()\n",
    "    progress_bar = tqdm(total=len(images))\n",
    "    with torch.no_grad():\n",
    "        for i in range(int(np.ceil(tensors.shape[0] / batch_size))):\n",
    "            # take a small batch\n",
    "            input_batch = tensors[i * batch_size:(i + 1) * batch_size]\n",
    "            progress_bar.update(input_batch.shape[0])\n",
    "            # move input to gpu\n",
    "            if torch.cuda.is_available() and use_cuda:\n",
    "                input_batch = input_batch.to('cuda')\n",
    "            # add outputs to a list\n",
    "            outputs.append(np.stack(model(input_batch).cpu().numpy(), axis=0))\n",
    "    model.to('cpu')\n",
    "    progress_bar.close()\n",
    "    return np.concatenate(outputs, axis=0)\n",
    "\n",
    "def compute_features_with_CLIP(dataset):\n",
    "    all_features = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(DataLoader(dataset, batch_size=100)):\n",
    "            features = clip_model.encode_image(images.to(device))\n",
    "            all_features.append(features)\n",
    "    return torch.cat(all_features).cpu().numpy()\n",
    "\n",
    "def normalize_feats(feat):\n",
    "    return feat/np.linalg.norm(feat, 2, axis=-1, keepdims=True)\n",
    "\n",
    "def compute_cosine_matrix(feats1, feats2):\n",
    "    feats1 = normalize_feats(feats1.reshape(feats1.shape[0], -1))\n",
    "    feats2 = normalize_feats(feats2.reshape(feats2.shape[0], -1))\n",
    "    return feats1.dot(feats2.T)\n",
    "\n",
    "def get_ordered_scores_matrix(smatrix, axis=1):\n",
    "    ordered_scores = np.flip(np.argsort(smatrix, axis=axis), axis=axis)\n",
    "    return ordered_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29936c1-de58-4c18-8bcb-507dfd97a2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" VARIABLES TO MODIFY \"\"\"\n",
    "model = resnet_50  # vgg, resnet_18, clip_model\n",
    "model_name = \"resnet_50\" # vgg, resnet_18, clip\n",
    "truncation = -3\n",
    "img_folder = 'Almageste'\n",
    "nb_nearest_neighbors = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b990fbfd-6c55-4d4e-b2f4-151a6a7559ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL DEFINITION\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(Model, self).__init__()\n",
    "        self.features = nn.Sequential(*list(original_model.children())[:truncation])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "def get_truncated_model():\n",
    "    if model_name == \"clip\":\n",
    "        return clip_model\n",
    "    conv4 = Model(model)\n",
    "    return conv4\n",
    "\n",
    "truncated_model = get_truncated_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b44f4ad-011c-4ed7-b03b-825e26e2c396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES COMPUTATION\n",
    "\n",
    "def compute_folder_feats(img_folder, model_name):\n",
    "    print(f\"Computing features for all the images in {img_folder}...\")\n",
    "    if model_name == \"clip\":\n",
    "        # features computation with CLIP\n",
    "        dataset = torchvision.datasets.ImageFolder(root=img_folder, transform=preprocess)\n",
    "        feats = compute_features_with_CLIP(dataset)\n",
    "        img_paths = []\n",
    "\n",
    "        for i in range(len(feats)):\n",
    "            data_sample, _ = dataset.samples[i]\n",
    "            img_paths.append(data_sample)\n",
    "        return feats, img_paths, \n",
    "    # features computation with other models\n",
    "    imgs, img_paths = get_images_list(img_folder)\n",
    "    feats = compute_features(truncated_model, imgs, size=320, batch_size=10, use_cuda=True)\n",
    "    return feats, img_paths\n",
    "\n",
    "feats, img_paths = compute_folder_feats(img_folder, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbf6204-0956-46d8-8d34-bc4660b00cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE/UPLOAD FEATURES\n",
    "feats_file = 'features'\n",
    "\n",
    "def save_feats(feats_file):\n",
    "    if os.path.isfile(f\"{feats_file}.npy\"):\n",
    "        os.remove(f\"{feats_file}.npy\")\n",
    "    # To save features as a binary file\n",
    "    np.save(f\"{feats_file}.npy\", feats)\n",
    "\n",
    "def load_feats(feats_file)\n",
    "    # To load already computed features\n",
    "    feats = np.load(f\"{feats_file}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fea90c-896b-411d-b44b-5121002f88ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY IMAGE DEFINITION\n",
    "\n",
    "# Some possible query images\n",
    "solar_eccentricity = \"toomer/toomer_almagest_p170_0.jpg\"\n",
    "eclipse = \"toomer/toomer_almagest_p272_0.jpg\"\n",
    "solar_equation = \"toomer/toomer_almagest_p174_0.jpg\" # and following 175, 178, 179 & 184\n",
    "eccentric_epicycle_equivalence = \"toomer/toomer_almagest_p165_0.jpg\"\n",
    "planet_anomaly = \"toomer/toomer_almagest_p170_0.jpg\" # not the right image\n",
    "\n",
    "\"\"\" VARIABLE TO MODIFY \"\"\"\n",
    "img_path = eccentric_epicycle_equivalence\n",
    "\n",
    "query_img = Image.open(img_path).convert('RGB')\n",
    "display_img(img_path, \"🔎 QUERY IMAGE 🔍\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c985d48-7aaa-4a45-91c5-0dd0c9d2ca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE RETRIEVAL\n",
    "def retrieve_img(query_img, model_name, nb_nn=nb_nearest_neighbors):\n",
    "    if model_name == \"clip\":\n",
    "        # Image retrieval with CLIP\n",
    "        query_img = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
    "        query_feat = clip_model.encode_image(query_img).cpu().detach().numpy()[0]\n",
    "        cos_sim = []\n",
    "        for i in range(len(features)):\n",
    "            cos_sim.append(dot(query_feat, features[i]) / (norm(query_feat) * norm(features[i])))\n",
    "        return (-np.array(cos_sim)).argsort()[:nb_nn]\n",
    "\n",
    "    # Compute features for the query image\n",
    "    query_feat = compute_features(truncated_model, [query_img], size=320, batch_size=10, use_cuda=True)\n",
    "    # Compute the difference angle between the query image and manuscript images in \n",
    "    cos_sim = compute_cosine_matrix(query_feat, feats)\n",
    "    # Find the nearest_neighbors of query diagram, from cosine metric point of view\n",
    "    return get_ordered_scores_matrix(cos_sim)[:, :nb_nn], cos_sim\n",
    "\n",
    "nearest_neighbors, cos_sim = retrieve_img(query_img, model_name, nb_nearest_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efc24d8-e4ae-4874-8073-de02273d4a88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DISPLAY NEAREST NEIGHBORS\n",
    "def display_nearest_neighbors(nearest_neighbors, cos_sim, model_name):\n",
    "    display_img(img_path, \"🔎 QUERY IMAGE 🔍\")\n",
    "    if model_name != \"clip\":\n",
    "        nearest_neighbors = nearest_neighbors[0]\n",
    "        cos_sim = cos_sim[0]\n",
    "    for neighbor in nearest_neighbors:\n",
    "        display_img(img_paths[neighbor], cos_sim[neighbor])\n",
    "\n",
    "display_nearest_neighbors(nearest_neighbors, cos_sim, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b91512-1446-4643-b837-f4079d2efcf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CSV GENERATION FOR GEPHI\n",
    "from csv import writer\n",
    "\n",
    "nn_csv = 'nn1.csv'\n",
    "def create_csv():\n",
    "    if os.path.isfile(nn_csv):\n",
    "        os.remove(nn_csv)\n",
    "    with open(nn_csv, mode=\"w\", newline='') as csvfile:\n",
    "        w_csv = csv.writer(csvfile, delimiter=',')\n",
    "        w_csv.writerow(['img_path', 'query_img', 'score'])\n",
    "        \n",
    "def append_to_csv(nearest_neighbors):\n",
    "    with open(nn_csv, mode=\"a\", newline='') as csvfile:\n",
    "        w_csv = csv.writer(csvfile)\n",
    "        for neighbor in nearest_neighbors[0]:\n",
    "            w_csv.writerow([img_paths[neighbor], img_path, score_matrix[0][neighbor]])\n",
    "\n",
    "create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383e414c-6d5a-4ba0-84f5-d12bb6810376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell with different query image results to visualize the diagrams that are the more detected as nn\n",
    "append_to_csv(nearest_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a744998-4359-4a57-b6f9-92db10a0ef07",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Diagrams exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96e691d-6de2-418a-b707-1747a1cabbf9",
   "metadata": {},
   "source": [
    "Now, our goal is to develop (as a team) a new way to navigate into our Astronomical Diagrams corpus: using clustering ([Gaussian mixture](https://scikit-learn.org/stable/modules/mixture.html), [Kmeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)), dimensionality reduction ([PCA](https://towardsdatascience.com/principal-component-analysis-for-dimensionality-reduction-115a3d157bad)), visualization tools ([t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) / [UMAP](https://umap-learn.readthedocs.io/en/latest/), Gephi, large collection dataviz), etc.!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f016b2c-7129-4c7d-b291-e39d3f6a745c",
   "metadata": {},
   "source": [
    "### Visualizations\n",
    "\n",
    "Examples of use of deep features to navigate into large-scale images / artworks collections:\n",
    "\n",
    "- [**PixPlot**](https://dhlab.yale.edu/projects/pixplot/) (Yale Digital Humanities Lab): [GitHub](https://github.com/YaleDHLab/pix-plot) / [Example](http://pixplot.yale.edu/v2/loc/)\n",
    "- [**Suprise machines**](https://www.jbe-platform.com/content/journals/10.1075/idj.22013.rod) (Harvard MetaLAB): [Twitter thread](https://twitter.com/dariorodighiero/status/1591074238378610690)\n",
    "- [**Nomic**](https://home.nomic.ai/): [MET with CLIP visual embeddings](https://atlas.nomic.ai/map/6f237809-3001-47a4-94da-2bc151cee88d/dad22af1-4ddf-4a0d-8105-f41d2e92b99a)\n",
    "- [**t-SNE Map**](https://experiments.withgoogle.com/t-sne-map) (Google Arts&Culture / MoMA)\n",
    "- [**imgs.ai**](https://imgs.ai/interface) (Fabian Offert)\n",
    "\n",
    "Other types of data-vizualisations:\n",
    "- [**Coins: Explore visually a large collection**](https://uclab.fh-potsdam.de/coins/): [Codebase](https://github.com/Flave/Muenzkabinett)\n",
    "- [**Gephi**](https://gephi.org/): Tutorials [[1]](https://docs.google.com/presentation/d/1tnFcRp6l6t64_DNyRZb0O18w1PFhEWA04awlHBNAOv0/edit?usp=sharing) & [[2]](https://docs.google.com/presentation/d/1Os0urAZyk8QQgw60nTgLlRVjbKgXykXta4uBrIKJoY4/edit?usp=sharing) → Try to map differently the columns for nodes/links in order to make apparent \n",
    "- [**D3**](https://d3js.org/): [Gallery](https://d3-graph-gallery.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31ae559-0f33-4147-a2cf-d32a4442aa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL IMPORT\n",
    "import time\n",
    "import warnings\n",
    "import pickle as pk\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from umap import UMAP\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bf1a7f-81ac-4c63-8f4f-d06d928dbbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" VARIABLES TO MODIFY \"\"\"\n",
    "n_dim = 1000\n",
    "cluster_nb = 25\n",
    "k_init = \"k-means++\" # \"random\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e0bf928-cf4d-460e-8481-2b39177467d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3766, 409600)\n"
     ]
    }
   ],
   "source": [
    "# DIMENSIONALITY REDUCTION\n",
    "def reduce_dim(feats, n_dim=n_dim):\n",
    "    # Flatten all the features, before applying dimensionality reduction\n",
    "    flat_feats = np.array([features.flatten() for features in feats])\n",
    "\n",
    "    # Features reduction using PCA: from 409600 dimensions to n dimensions\n",
    "    pca = PCA(n_components=n_dim, random_state=22)\n",
    "    pca.fit(flat_feats)\n",
    "    reduced_feats = pca.transform(flat_feats)\n",
    "    print(f\"Flatten features: {flat_feats.shape} dimensions\" + \"\\n\" +\n",
    "          f\"Reduced features: {reduced_feats.shape} dimensions\")\n",
    "    print(pca.explained_variance_ratio_[:10])\n",
    "    print(np.sum(pca.explained_variance_ratio_))\n",
    "    return reduced_feats\n",
    "\n",
    "reduced_feats = reduce_dim(feats, n_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc17a9-eee3-4060-8a83-c0c4f4bc7a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE/UPLOAD PCA\n",
    "pca_file = 'pca_1000'\n",
    "\n",
    "def save_pca(pca_file):\n",
    "    # Save PCA result as binary file\n",
    "    pk.dump(pca, open(f\"{pca_file}.pkl\", \"wb\"))\n",
    "    pk.dump(reduced_feats, open(f\"{pca_file}_results.pkl\", \"wb\"))\n",
    "\n",
    "def load_pca(pca_file):\n",
    "    reduced_feats = np.load(f\"{pca_file}_results.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cce84f5-112e-4aae-a1f1-a89352119e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAUSSIAN MIXTURE CLUSTERING\n",
    "def gaussian_mix(reduced_feats, cluster_nb=cluster_nb):\n",
    "    '''\n",
    "    GaussianMixture() parameters\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html\n",
    "    '''\n",
    "    gmixture = GaussianMixture(cluster_nb, random_state=22)\n",
    "    gmixture.fit(reduced_feats)\n",
    "    gmixture_labels = gmixture.predict(reduced_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27089b92-1e27-40f5-a28b-dd50c6853bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMEANS CLUSTERING\n",
    "def k_means(reduced_feats, cluster_nb=cluster_nb):\n",
    "    '''\n",
    "    KMeans() parameters\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "    '''\n",
    "    kmeans = KMeans(cluster_nb, random_state=22)\n",
    "    kmeans.fit(reduced_feats)\n",
    "    centroid_distances = np.empty(shape=(cluster_nb, cluster_nb)) \n",
    "\n",
    "    for i in range(cluster_nb):\n",
    "        for j in range(cluster_nb):\n",
    "            cluster_center = kmeans.cluster_centers_\n",
    "            # compute euclidian distances between all clusters centroids\n",
    "            centroid_distances[i][j] = np.linalg.norm(cluster_center[i] - cluster_center[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9068e273-e8b5-4cf3-9d6e-1d8c29a10db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITHIN-CLUSTER-SUM-OF-SQUARES\n",
    "def plot_elbow_graph(wcss):\n",
    "    \"\"\"\n",
    "    To choose the number of clusters that is pertinent for the dataset\n",
    "    \"\"\"\n",
    "    plt.plot(range(1,21), wcss)\n",
    "    plt.title('The Elbow Method Graph')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('WCSS')\n",
    "    plt.show()\n",
    "    \n",
    "def wcss(reduced_feats):\n",
    "    wcss = []\n",
    "    # this loop will fit the k-means algorithm to our data and \n",
    "    # second we will compute the within cluster sum of squares and happended to our wcss list.\n",
    "    for i in range(1, 21):\n",
    "        kmeans = KMeans(n_clusters=i, init=k_init, max_iter=300, n_init=10, random_state=0)\n",
    "        # n_init which is the number of times the K_means will be run with different initial centroid\n",
    "        kmeans.fit(reduced_feats)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "        # inertia_ = sum of squared distances of samples to their closest cluster center\n",
    "    plot_elbow_graph(wcss)\n",
    "    \n",
    "wcss(reduced_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d517ed0-f98c-472e-a4b6-beb8d882ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=cluster_nb, init=k_init, max_iter=3000)\n",
    "kmeans_feats = kmeans.fit_predict(reduced_feats)\n",
    "print(kmeans_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e373508b-3212-4c45-a381-191274fb9583",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install ipympl\n",
    "#%matplotlib notebook\n",
    "\n",
    "#Getting unique labels\n",
    "#%matplotlib notebook\n",
    "u_labels = np.unique(kmeans_feats)\n",
    "centroids = model.cluster_centers_\n",
    " \n",
    "#plotting the results:\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "ax = fig.add_subplot(111, projection='3d') \n",
    "\n",
    "for i in u_labels:\n",
    "    ax.scatter(reduced_feats[kmeans_feats==i, 0], reduced_feats[kmeans_feats==i, 1], label=i, s=10)\n",
    "plt.scatter(centroids[:,0], centroids[:,1], s=15, color='k')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319831b0-3b15-4ed9-8e74-d34070f26ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nearest neighbours\n",
    "score_center = []\n",
    "for i in range(centroids.shape[0]) :\n",
    "    index = np.where(kmeans_feats==i)    \n",
    "    score_center.append(compute_cosine_matrix(centroids[i],reduced_feats[kmeans_feats==i, :]))  # computes the cosine similarity score matri\n",
    "center_diagram = get_ordered_scores_matrix(score_center[0], axis=1)[:,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552bef16-3c43-4bf3-be61-47b020cc57c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_order = np.argsort(centroid_distances)[0] #ordering the clusters according to their distance to the first one - in euclidian distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfc9813-50d7-4470-a059-e2b7ca91746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dictionnary to link each cluster to its corresponding images indexes \n",
    "\n",
    "groups = {}\n",
    "for i, cluster in enumerate(kmeans_feats.labels_):\n",
    "    if cluster not in groups.keys():\n",
    "        groups[cluster] = []\n",
    "        groups[cluster].append(i)\n",
    "    else:\n",
    "        groups[cluster].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f747352d-52c2-4d6a-a593-984a5dd1f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "cos_sim = []\n",
    "\n",
    "for i in range(len(features)):\n",
    "    cos_sim.append(dot(query_feat, features[i])/(norm(query_feat)*norm(features[i])))\n",
    "\n",
    "nearest_neighbors = (-np.array(cos_sim)).argsort()[:number_nn]\n",
    "\n",
    "\n",
    "#nearest_neighbors = []\n",
    "#for cluster_id in range(n_clust):\n",
    "#    cos_sim = []\n",
    "#    for i in range(len(groups[cluster_id])):\n",
    "#        feat = reduced_feats[groups[cluster_id][i]]\n",
    "#        cos_sim.append(dot(centroids[cluster_id], feat)/(norm(centroids[cluster_id])*norm(feat)))\n",
    "#\n",
    "#    nearest_neighbors.append((-np.array(cos_sim)).argsort()[:5]) #mettre arguments dans argsort good luck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4024d4f1-d949-46b4-acc2-7e8da1d0c857",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the 5 first images of each cluster, according to the cluster order: the closest clusters of the first one, according to euclidian distance\n",
    "images, images_path = get_images_list('Diagrams') \n",
    "\n",
    "for i in cluster_order:\n",
    "    print(f'Cluster {i}, size: {len(groups[i])}')\n",
    "    for j in range(min(5,len(groups[i]))):\n",
    "        print(images_path[groups[i][j]])\n",
    "        display(images[groups[i][j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dca97f-2792-413b-aa18-da5f8bd2e35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#T-SNE reduction in 2D (starting with the features reduced by PCA)\n",
    "\n",
    "'''\n",
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2)\n",
    "tsne_embedding = tsne.fit_transform(reduced_feats)\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06173ac-40bc-46ba-9c36-1dd4b526bd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UMAP reduction in 2D (starting with the features reduced by PCA)\n",
    "\n",
    "reducer = UMAP()\n",
    "umap_embedding = reducer.fit_transform(reduced_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b524f1-ea6e-4600-8e43-5cc4dccf8388",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each cluster point, save its corresponding UMAP coordinates\n",
    "\n",
    "groups_umap = {}\n",
    "for umap, cluster in zip(umap_embedding, kmeans_feats.labels_):\n",
    "    if cluster not in groups_umap.keys():\n",
    "        groups_umap[cluster] = []\n",
    "        groups_umap[cluster].append([umap[0],umap[1]])\n",
    "    else:\n",
    "        groups_umap[cluster].append([umap[0],umap[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4540875-97f9-4207-9002-ff0aa7f4d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 30))\n",
    "for i in range(len(groups_umap)):\n",
    "    array = np.array(groups_umap[i])\n",
    "    plt.scatter(array[:,0],array[:,1], label=f'{i}', color=(np.random.random(), np.random.random(), np.random.random()))\n",
    "    for j in range(min(10,len(array))):\n",
    "        plt.annotate(i, (array[j][0], array[j][1]))\n",
    "\n",
    "plt.xlabel(\"umap-first-component\")\n",
    "plt.ylabel(\"umap-second-component\")\n",
    "plt.title('UMAP 2D reduction')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd9b2f0-2aa8-4c7e-b8f9-fcb654c70105",
   "metadata": {},
   "source": [
    "# Cluster analysis / understanding diagrams typology -> creation of a chosen critical edition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff88012-5162-446d-a8cc-b8f7e948d5a7",
   "metadata": {},
   "source": [
    "## First, a proper plot (in grid format) of the clusters we are looking at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3089b791-99ab-4ef5-88a7-eb405f3f968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Small function to display PIL images in Grid -> to visualize easely our clusters of interest\n",
    "\n",
    "def display_images(\n",
    "    images, \n",
    "    columns=5, width=20, height=8, max_images=50, \n",
    "    label_wrap_length=50, label_font_size=8):\n",
    "\n",
    "    if not images:\n",
    "        print(\"No images to display.\")\n",
    "        return \n",
    "\n",
    "    if len(images) > max_images:\n",
    "        print(f\"Showing {max_images} images of {len(images)}:\")\n",
    "        images=images[0:max_images]\n",
    "\n",
    "    height = max(height, int(len(images)/columns) * height)\n",
    "    plt.figure(figsize=(width, height))\n",
    "    for i, image in enumerate(images):\n",
    "        \n",
    "        plt.subplot(int(len(images) / columns + 1), columns, i + 1)\n",
    "        plt.imshow(image)\n",
    "\n",
    "        if hasattr(image, 'filename'):\n",
    "            title=image.filename\n",
    "            plt.title(title, fontsize=label_font_size);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb7b6fc-7e1e-4152-a722-2a89035d1206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of lists of PIL images corresponding to each cluster\n",
    "\n",
    "images_cluster = [[] for _ in range(len(groups))]\n",
    "\n",
    "for k in range(len(groups)):\n",
    "    for i in range(len(groups[k])):\n",
    "        images[groups[k][i]].filename = images_path[groups[k][i]]\n",
    "        images_cluster[k].append(images[groups[k][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c598f3-75f2-434b-8459-9e1fa5a337a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display cluster of interest\n",
    "\n",
    "cluster_id = 0\n",
    "\n",
    "display_images(images_cluster[cluster_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d7844c-8d3b-41de-84af-b39f07547dd1",
   "metadata": {},
   "source": [
    "## Then, possible use of (more or less simple) image processing to analyze the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e209fa-d581-439f-be82-62699fd146ea",
   "metadata": {},
   "source": [
    "### Mean image of each cluster? (simple, but certainly not very useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0a1479-4466-4169-b407-186e5e0d41cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resize all images of interest\n",
    "\n",
    "images_analysis = []\n",
    "\n",
    "cluster_id = 0\n",
    "\n",
    "w, h = 200, 200\n",
    "\n",
    "for i in range(len(images_cluster[cluster_id])):\n",
    "    images_analysis.append(images_cluster[cluster_id][i].resize((w,h)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330c9e23-0155-4a93-875c-7ed0655011a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's just try creating a \"mean\" image of our cluster\n",
    "\n",
    "# Create a numpy array of floats to store the average (assume RGB images)\n",
    "arr = np.zeros((h,w,3),np.float)\n",
    "\n",
    "# Build up average pixel intensities, casting each image as an array of floats\n",
    "for im in images_analysis:\n",
    "    imarr = np.array(im,dtype=np.float)\n",
    "    arr = arr+imarr/len(images_analysis)\n",
    "\n",
    "# Round values in array and cast as 8-bit integer\n",
    "arr = np.array(np.round(arr),dtype=np.uint8)\n",
    "\n",
    "# Generate, save and preview final image\n",
    "out = Image.fromarray(arr,mode=\"RGB\")\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92b604f-e8f1-4795-9702-2727d22846b5",
   "metadata": {},
   "source": [
    "## Test of feature matching - homography mapping on all diagrams of a cluster?\n",
    "\n",
    "Some examples / explanations of OpenCV homography / use of RANSAC to estimate homographies:\n",
    "\n",
    "• https://docs.opencv.org/4.x/d9/dab/tutorial_homography.html\n",
    "\n",
    "• https://docs.opencv.org/3.4/d1/de0/tutorial_py_feature_homography.html\n",
    "\n",
    "• https://learnopencv.com/homography-examples-using-opencv-python-c/\n",
    "\n",
    "• Some explanation slides: http://6.869.csail.mit.edu/fa12/lectures/lecture13ransac/lecture13ransac.pdf\n",
    "\n",
    "**Paper of interest:**\n",
    "\n",
    "• RANSAC-Flow: generic two-stage image alignment (Xi Shen): https://imagine.enpc.fr/~shenx/RANSAC-Flow/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab82fc5e-7c8e-452e-9b47-517692887429",
   "metadata": {},
   "source": [
    "## Test of binarization of the images?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e390f89-b582-47fa-b78c-64bbebc59fc3",
   "metadata": {},
   "source": [
    "## Create a critical edition without image processing tools?\n",
    "\n",
    "• Geogebra: https://www.geogebra.org/?lang=en"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
